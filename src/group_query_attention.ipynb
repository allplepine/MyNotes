{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b07f60a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from typing import Optional, Union, Any"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "4e3a7cb1",
   "metadata": {},
   "outputs": [],
   "source": [
    "class GroupQueryAttention(nn.Module):\n",
    "    \"\"\"\n",
    "    Group Query Attention (GQA) 实现\n",
    "    \n",
    "    GQA是一种改进的注意力机制，通过将注意力头分组来减少计算复杂度。\n",
    "    它将所有的查询头(Q)保留，但将键(K)和值(V)头分组，每组共享相同的K和V。\n",
    "    \n",
    "    主要优势：\n",
    "    1. 减少内存使用：K和V的参数量减少\n",
    "    2. 提高计算效率：减少矩阵乘法操作\n",
    "    3. 保持性能：在大多数任务上表现接近标准多头注意力\n",
    "    \"\"\"\n",
    "    def __init__(self, d_model, n_heads, n_groups):\n",
    "        '''\n",
    "        Group Query Attention 初始化\n",
    "        \n",
    "        Args:\n",
    "            d_model: 模型的隐藏维度，即输入特征的维度\n",
    "            n_heads: 注意力头的总数量\n",
    "            n_groups: 分组数量，每组共享相同的K和V投影\n",
    "        '''\n",
    "        super().__init__()\n",
    "        # 确保维度能够正确分组\n",
    "        assert d_model % n_groups == 0, \"d_model必须能被n_groups整除\"\n",
    "        assert n_heads % n_groups == 0, \"n_heads必须能被n_groups整除\"\n",
    "        \n",
    "        self.d_model = d_model\n",
    "        self.n_heads = n_heads\n",
    "        self.n_groups = n_groups\n",
    "        \n",
    "        # 计算每个头的维度\n",
    "        self.head_dim = d_model // n_heads\n",
    "        # 缩放因子，用于防止梯度消失\n",
    "        self.scale = self.head_dim ** -0.5\n",
    "        # 每组中的头数量\n",
    "        self.n_heads_per_group = n_heads // n_groups\n",
    "        \n",
    "        # 投影层定义\n",
    "        # Q投影：所有头都有独立的Q投影\n",
    "        self.q_proj = nn.Linear(d_model, d_model, bias=False)\n",
    "        # K投影：每组共享一个K投影，所以维度是 n_heads_per_group * head_dim\n",
    "        self.k_proj = nn.Linear(d_model, self.n_heads_per_group * self.head_dim, bias=False)\n",
    "        # V投影：每组共享一个V投影，所以维度是 n_heads_per_group * head_dim\n",
    "        self.v_proj = nn.Linear(d_model, self.n_heads_per_group * self.head_dim, bias=False)\n",
    "        # 输出投影层\n",
    "        self.out_proj = nn.Linear(d_model, d_model, bias=False)\n",
    "        \n",
    "    def forward(self, \n",
    "                x: torch.Tensor, \n",
    "                mask: Optional[torch.Tensor] = None, \n",
    "                dropout: float = 0.0\n",
    "        ):\n",
    "        \"\"\"\n",
    "        前向传播\n",
    "        \n",
    "        Args:\n",
    "            x: 输入张量，形状为 (batch_size, seq_len, d_model)\n",
    "            mask: 可选的注意力掩码，形状为 (seq_len, seq_len)\n",
    "            dropout: dropout概率\n",
    "            \n",
    "        Returns:\n",
    "            attn_output: 注意力输出，形状为 (batch_size, seq_len, d_model)\n",
    "        \"\"\"\n",
    "        batch_size, seq_len, _ = x.shape\n",
    "        \n",
    "        # 步骤1: 线性投影得到Q、K、V\n",
    "        # Q: 所有头都有独立的投影\n",
    "        q = self.q_proj(x)  # (batch_size, seq_len, d_model)\n",
    "        # K: 每组共享投影，所以维度较小\n",
    "        k = self.k_proj(x)  # (batch_size, seq_len, n_heads_per_group * head_dim)\n",
    "        # V: 每组共享投影，所以维度较小\n",
    "        v = self.v_proj(x)  # (batch_size, seq_len, n_heads_per_group * head_dim)\n",
    "        \n",
    "        # 步骤2: 重塑张量维度\n",
    "        # Q: 重塑为多头格式\n",
    "        q = q.view(batch_size, seq_len, self.n_heads, self.head_dim).transpose(1, 2)\n",
    "        # 形状: (batch_size, n_heads, seq_len, head_dim)\n",
    "        \n",
    "        # K: 重塑为分组格式\n",
    "        k = k.view(batch_size, seq_len, self.n_groups, self.head_dim).transpose(1, 2)\n",
    "        # 形状: (batch_size, n_groups, seq_len, head_dim)\n",
    "        \n",
    "        # V: 重塑为分组格式\n",
    "        v = v.view(batch_size, seq_len, self.n_groups, self.head_dim).transpose(1, 2)\n",
    "        # 形状: (batch_size, n_groups, seq_len, head_dim)\n",
    "        \n",
    "        # 步骤3: 扩展K和V以匹配所有头\n",
    "        # 将每组共享的K扩展给该组的所有头\n",
    "        k = k[:,:,None,:,:].expand(-1, -1, self.n_heads_per_group, -1, -1).reshape(batch_size, self.n_heads, seq_len, self.head_dim)\n",
    "        # 将每组共享的V扩展给该组的所有头\n",
    "        v = v[:,:,None,:,:].expand(-1, -1, self.n_heads_per_group, -1, -1).reshape(batch_size, self.n_heads, seq_len, self.head_dim)\n",
    "        \n",
    "        # 步骤4: 计算注意力权重\n",
    "        # Q @ K^T: 计算查询和键的相似度\n",
    "        attn_weights = q @ k.transpose(-2, -1) * self.scale\n",
    "        # 形状: (batch_size, n_heads, seq_len, seq_len)\n",
    "\n",
    "        # 步骤5: 应用掩码（如果提供）\n",
    "        if mask is not None:\n",
    "            # 将掩码应用到注意力权重上，被掩码的位置设为负无穷\n",
    "            attn_weights = attn_weights.masked_fill(mask == 0, float(\"-inf\"))\n",
    "        \n",
    "        # 步骤6: Softmax归一化\n",
    "        attn_weights = attn_weights.softmax(dim=-1)\n",
    "        \n",
    "        # 步骤7: 应用dropout\n",
    "        attn_weights = F.dropout(attn_weights, p=dropout)\n",
    "        \n",
    "        # 步骤8: 计算注意力输出\n",
    "        attn_output = attn_weights @ v\n",
    "        # 形状: (batch_size, n_heads, seq_len, head_dim)\n",
    "        \n",
    "        # 步骤9: 重塑并应用输出投影\n",
    "        # 将多头输出合并\n",
    "        attn_output = attn_output.transpose(1, 2).contiguous().view(batch_size, seq_len, self.d_model)\n",
    "        # 应用最终的线性投影\n",
    "        return self.out_proj(attn_output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8e21f1f6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 1024, 1024])\n"
     ]
    }
   ],
   "source": [
    "gqa = GroupQueryAttention(d_model=1024, n_heads=16, n_groups=4)\n",
    "x = torch.randn(1, 1024, 1024)\n",
    "mask = torch.tril(torch.ones(1024, 1024))\n",
    "output = gqa(x, mask)\n",
    "print(output.shape)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "gpt",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
